{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Dict, List,Any\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from pandas import DataFrame\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from utils.const import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cli():\n",
    "    mjs = \"\"\"\n",
    "    _________.__              .__  .__  __  .__        ___________.__\n",
    "    ____________________  _____       _____  __________________________   \n",
    "    \\______   \\______   \\/  _  \\     /     \\ \\_   _____|__    ___/  _  \\  \n",
    "     |       _/|     ___/  /_\\  \\   /  \\ /  \\ |    __)_  |    | /  /_\\  \\ \n",
    "     |    |   \\|    |  /    |    \\ /    Y    \\|        \\ |    |/    |    \\\\\n",
    "     |____|_  /|____|  \\____|__  / \\____|__  /_______  / |____|\\____|__  /\n",
    "            \\/                 \\/          \\/        \\/                \\/ \n",
    "   \"\"\"\n",
    "    print(mjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_profile(driver,curren_url: str):\n",
    "    # obtener el codigo de la pagina\n",
    "    src = driver.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    #Obtener el ur de la imagen\n",
    "    div = soup.find('div', class_='pv-top-card__non-self-photo-wrapper ml0')\n",
    "    img = div.find('img')['src'] if div else \"No hay imagen\"\n",
    "    nombre = soup.find('h1', class_='text-heading-xlarge inline t-24 v-align-middle break-words')\n",
    "    nombre = nombre.text.strip() if nombre else \"No hay nombre\"\n",
    "    description = soup.find('div', class_= 'text-body-medium break-words')\n",
    "    description = description.text.strip() if description else \"No hay descripcion\"\n",
    "    empresa_actual = soup.find('div', class_='inline-show-more-text inline-show-more-text--is-collapsed inline-show-more-text--is-collapsed-with-line-clamp inline')\n",
    "    empresa_actual = empresa_actual.text.strip() if empresa_actual else \"No hay empresa actual\"\n",
    "    about = soup.find('section', class_='artdeco-card ember-view relative break-words pb3 mt2')\n",
    "    if about:\n",
    "      about = about.find('div', class_='inline-show-more-text inline-show-more-text--is-collapsed inline-show-more-text--is-collapsed-with-line-clamp full-width')\n",
    "      about = about.text.strip() if about else \"No hay about\"\n",
    "      about = about.replace(\"… ver más\", \"\")\n",
    "    \n",
    "    experiencias = soup.find_all('div',class_=\"pvs-list__outer-container\")\n",
    "    \n",
    "    exp = [{\n",
    "    \"puesto\": experiencia.find('div', class_='display-flex align-items-center mr1 t-bold').text.strip() if experiencia.find('div', class_='display-flex align-items-center mr1 t-bold') else \"No hay puesto\",\n",
    "    \"img\": experiencia.find('img')['src'] if experiencia.find('img') else \"No hay imagen\",\n",
    "    \"empresa\": experiencia.find('span', class_='t-14 t-normal').text.strip() if experiencia.find('span', class_='t-14 t-normal') else \"No hay empresa\",\n",
    "    \"fecha\": experiencia.find('span', class_='t-14 t-normal t-black--lightl').text.strip() if experiencia.find('span', class_='t-14 t-normal t-black--lightl') else \"No hay fecha\"\n",
    "    }\n",
    "    for experiencia in experiencias\n",
    "      if experiencia.find('div', class_='display-flex align-items-center mr1 t-bold') and experiencia.find('div', class_='display-flex align-items-center mr1 t-bold').text.strip() != \"No hay puesto\"\n",
    "    ]\n",
    "    conocimientos_aptitudes = soup.find_all('div', class_='pvs-entity pvs-entity--padded pvs-list__item--no-padding-in-columns')\n",
    "    conocimiento = [{\n",
    "      \"nombre\": conocimiento_aptitud.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold').text.strip() if conocimiento_aptitud.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold') else \"No hay nombre\",\n",
    "      \"descripcion\": conocimiento_aptitud.find('div', class_='inline-show-more-text inline-show-more-text--is-collapsed inline-show-more-text--is-collapsed-with-line-clamp full-width').text.strip() if conocimiento_aptitud.find('div', class_='inline-show-more-text inline-show-more-text--is-collapsed inline-show-more-text--is-collapsed-with-line-clamp full-width') else \"No hay descripcion\",\n",
    "    }\n",
    "    for conocimiento_aptitud in conocimientos_aptitudes\n",
    "    ]\n",
    "    info_idiomas = soup.find_all('section', class_='artdeco-card ember-view relative break-words pb3 mt2')\n",
    "    info_idiomas = [idioma for idioma in info_idiomas if idioma.find('div', id='languages')]\n",
    "    idiomas = []\n",
    "    for idioma in info_idiomas:\n",
    "        items = idioma.find_all('li', class_='artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column')\n",
    "        for item in items:\n",
    "            nombre_idioma = item.find('div', class_='display-flex align-items-center mr1 t-bold').text.strip() if item.find('div', class_='display-flex align-items-center mr1 t-bold') else \"No hay idioma\"\n",
    "            nivel = item.find('span', class_='t-14 t-normal t-black--light').text.strip() if item.find('span', class_='t-14 t-normal t-black--light') else \"No hay nivel\"\n",
    "            idioma_data = {\n",
    "                \"idioma\": nombre_idioma,\n",
    "                \"nivel\": nivel\n",
    "            }\n",
    "            idiomas.append(idioma_data)\n",
    "    educaciones = soup.find_all('section', class_='artdeco-card ember-view relative break-words pb3 mt2')\n",
    "    educaciones = [edu for edu in educaciones if edu.find('div', id='education')]\n",
    "    educacion = []\n",
    "    for edu in educaciones:\n",
    "        items = edu.find_all('li', class_='artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column')\n",
    "        for item in items:\n",
    "            img = item.find('img')['src'] if item.find('img') else \"No hay imagen\"\n",
    "            escuela = item.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold').text.strip() if item.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold') else \"No hay nombre\"\n",
    "            especialidad = item.find('span', class_='t-14 t-normal').text.strip() if item.find('span', class_='pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal') else \"No hay especialidad\"\n",
    "            educacion_data = {\n",
    "                \"img\": img,\n",
    "                \"escuela\": escuela,\n",
    "                \"especialidad\": especialidad\n",
    "            }\n",
    "            \n",
    "            educacion.append(educacion_data)\n",
    "  \n",
    "    licencias_certificaciones = soup.find_all('section', class_='artdeco-card ember-view relative break-words pb3 mt2')\n",
    "    licencias = []\n",
    "    for licencia in licencias_certificaciones:\n",
    "        if licencia.find('div', id='licenses_and_certifications'):\n",
    "            items = licencia.find_all('li', class_='artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column')\n",
    "            for item in items:\n",
    "                img = item.find('img')['src'] if item.find('img') else \"No hay imagen\"\n",
    "                nombre = item.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold').text.strip() if item.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold') else \"No hay nombre\"\n",
    "                plataforma = item.find('span', class_='t-14 t-normal').text.strip() if item.find('span', class_='t-14 t-normal') else \"No hay plataforma\"\n",
    "                expedicion = item.find('span', class_='t-14 t-normal t-black--light').text.strip() if item.find('span', class_='t-14 t-normal t-black--light') else \"No hay expedicion\"\n",
    "                URL_certificacion = item.find('a')['href'] if item.find('a') else \"No hay URL\"\n",
    "                licencia_data = {\n",
    "                    \"img\": img,\n",
    "                    \"nombre\": nombre,\n",
    "                    \"plataforma\": plataforma,\n",
    "                    \"expedicion\": expedicion,\n",
    "                    \"URL_certificacion\": URL_certificacion\n",
    "                }\n",
    "                licencias.append(licencia_data)\n",
    "    info = {\n",
    "       \"URL\": curren_url,\n",
    "        \"nombre\": nombre,\n",
    "        \"img\": img,\n",
    "        \"description\": description,\n",
    "        \"empresa_actual\": empresa_actual,\n",
    "        \"about\": about if about else \"No hay about\",\n",
    "        \"idiomas\": idiomas if idiomas else \"No hay idiomas\",\n",
    "        \"experiencias\": exp if exp else \"No hay experiencias\",\n",
    "        \"conocimientos_aptitudes\" : conocimiento if conocimiento else \"No hay conocimientos y aptitudes\",\n",
    "        \"educacion\": educacion if educacion else \"No hay educacion\",\n",
    "        \"licencias_certificaciones\": licencias if licencias else \"No hay licencias y certificaciones\"\n",
    "    }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_data_from_profile(driver,urls_profiles: List[str]):\n",
    "  data_profiles = []\n",
    "  for url in urls_profiles:\n",
    "      driver.get(url)\n",
    "      try:\n",
    "         WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'pv-top-card--list')))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error en {url}\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "      #ir hasta abajo de la pagina\n",
    "      driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "      time.sleep(5)\n",
    "      data_profiles.append(get_one_profile(driver,url))\n",
    "  return data_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_by_keyword(driver,config: Dict[str, str],urls_profiles: List[str]):\n",
    "      global user_id\n",
    "      keywords = config[\"keyword\"]\n",
    "      location = config[\"location\"]\n",
    "      initial_page = config[\"initial_page\"]\n",
    "      final_page = config[\"final_page\"]\n",
    "    # Abrir la página de búsqueda\n",
    "      search_url = \"https://www.google.com\"\n",
    "      driver.get(search_url)\n",
    "       # Esperar hasta que el campo de entrada de búsqueda esté presente en la página\n",
    "      input_search = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "\n",
    "      input_search.send_keys(f\"site:linkedin.com/in/ AND {keywords} AND {location}\")\n",
    "      input_search.submit()\n",
    "\n",
    "      for _ in range(final_page):\n",
    "             # Esperar hasta que aparezca el elemento de resultados de búsqueda en la página\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'rcnt')))\n",
    "\n",
    "              # Obtener el código fuente de la página de resultados de búsqueda\n",
    "        src = driver.page_source\n",
    "\n",
    "              #pasear el html\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        profiles = soup.find_all('div', class_='MjjYud')\n",
    "\n",
    "        for profile in profiles:\n",
    "          profile_linkedin_url = profile.find('a')['href']\n",
    "          try:\n",
    "            if profile_linkedin_url.startswith('/search'):\n",
    "              break\n",
    "            urls_profiles.append(profile_linkedin_url)\n",
    "          except Exception as e:\n",
    "            insert_error({\n",
    "            \"timestamp\": format_timestamp(time.time()),\n",
    "            \"state\": \"scrapping\",\n",
    "            \"error_message\": \"error al obtener el url del perfil\",\n",
    "            \"request_url\": search_url,\n",
    "            \"stack_trace\": str(e),\n",
    "            \"additional_info\": \"profile_linkedin_url\",\n",
    "            \"user_id\": user_id,})\n",
    "            continue\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        try:\n",
    "          next_button = driver.find_element(By.ID, 'pnnext')\n",
    "          next_button.click()\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          print(urls_profiles)\n",
    "          continue\n",
    "      print(f\"Se obtuvieron {len(urls_profiles)} urls de perfiles\")\n",
    "      info = scrap_data_from_profile(driver, urls_profiles)\n",
    "      df = pd.DataFrame(info)\n",
    "      return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_logged(driver):\n",
    "    #ver si hay un elemento con la clase form__input--text input_verification_pin\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'form__input--text input_verification_pin')\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def insert_error(error):\n",
    "  headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "  }\n",
    "  j = json.dumps(error)\n",
    "  return requests.post(\"http://localhost:8787/error\", data=j, headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_urls(driver,urls_profiles: List[str]):\n",
    "    info = scrap_data_from_profile(driver, urls_profiles)\n",
    "    df = pd.DataFrame(info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def format_timestamp(timestamp: float) -> str:\n",
    "    \"\"\"\n",
    "    Convierte un timestamp en una cadena con formato \"YYYY-MM-DD HH:MM:SS\".\n",
    "\n",
    "    Args:\n",
    "        timestamp (float): El timestamp a formatear.\n",
    "\n",
    "    Returns:\n",
    "        str: La cadena formateada.\n",
    "    \"\"\"\n",
    "    datetime_obj = datetime.fromtimestamp(timestamp)\n",
    "    formatted_datetime = datetime_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return formatted_datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scraper:  \n",
    "    def search_people(config: Dict[str, str]) -> DataFrame:\n",
    "      global user_id\n",
    "      init_cli()\n",
    "      \n",
    "      is_search:bool = config[\"is_search\"]\n",
    "      profiles_to_search:List[str] = config[\"profiles_to_search\"]\n",
    "      urls_profiles = []\n",
    "      df:DataFrame = pd.DataFrame()\n",
    "      driver = None\n",
    "      # Crear una instancia\n",
    "      print(CHROME_DRIVER_PATH)\n",
    "      try:\n",
    "        driver = webdriver.Chrome(\"C:\\\\Users\\\\urieh\\\\Documents\\\\FORTE\\\\META\\\\chromedriver-win64\\\\chromedriver.exe\")\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error al crear el driver\")\n",
    "        error =insert_error({\n",
    "          \"timestamp\": format_timestamp(time.time()),\n",
    "          \"state\": \"unlogged\",\n",
    "          \"error_message\": \"Error al crear el driver\",\n",
    "          \"request_url\": \"No aplica\",\n",
    "          \"stack_trace\": \"No aplica\",\n",
    "          \"additional_info\": \"No se pudo crear el driver\",\n",
    "          \"user_id\": user_id\n",
    "        })\n",
    "        print(error)\n",
    "        return\n",
    "      # Logging into LinkedIn\n",
    "      driver.get(LOGIN_URL)\n",
    "      time.sleep(2)\n",
    "\n",
    "      user = os.getenv(\"USERNAME_LINKEDIN\")\n",
    "      password = os.getenv(\"PASSWORD_LINKEDIN\")\n",
    "      username = driver.find_element(By.ID, \"username\")\n",
    "      username.send_keys(user)\n",
    "\n",
    "      pword = driver.find_element(By.ID, \"password\")\n",
    "      pword.send_keys(password)\n",
    "\n",
    "      driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "      time.sleep(2)\n",
    "      if is_logged(driver) is False:\n",
    "        insert_error({\n",
    "          \"timestamp\": format_timestamp(time.time()),\n",
    "          \"state\": \"unlogged\",\n",
    "          \"error_message\": \"No se pudo iniciar sesion, ingrese primero a linkedin\",\n",
    "          \"request_url\": \"No aplica\",\n",
    "          \"stack_trace\": \"No aplica\",\n",
    "          \"additional_info\": \"No se pudo iniciar sesion\",\n",
    "          \"user_id\": user_id\n",
    "        })\n",
    "        #terminar la ejecucion\n",
    "        driver.quit()\n",
    "        return\n",
    "      if is_search:\n",
    "        df = search_by_urls(driver,profiles_to_search)\n",
    "      else:\n",
    "        df = search_by_keyword(driver,config,urls_profiles)\n",
    "      driver.quit()\n",
    "      return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "def insert_config(config:Dict[str, Any]) -> Dict[str, Any]:\n",
    "    #convertir is_search de boolean a numero\n",
    "  config[\"is_search\"] = 0 if config[\"is_search\"] is False else 1\n",
    "  #convertir profiles_to_search a string\n",
    "  config[\"profile_url\"] = \",\".join(config[\"profiles_to_search\"])\n",
    "  headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "  }\n",
    "  json_data = json.dumps(config)\n",
    "  request = requests.post(\"http://127.0.0.1:8787/search\", headers=headers, data=json_data)\n",
    "  return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _________.__              .__  .__  __  .__        ___________.__\n",
      "    ____________________  _____       _____  __________________________   \n",
      "    \\______   \\______   \\/  _  \\     /     \\ \\_   _____|__    ___/  _  \\  \n",
      "     |       _/|     ___/  /_\\  \\   /  \\ /  \\ |    __)_  |    | /  /_\\  \\ \n",
      "     |    |   \\|    |  /    |    \\ /    Y    \\|        \\ |    |/    |    \\\n",
      "     |____|_  /|____|  \\____|__  / \\____|__  /_______  / |____|\\____|__  /\n",
      "            \\/                 \\/          \\/        \\/                \\/ \n",
      "   \n",
      "C:\\Users\\urieh\\Documents\\FORTE\\META\\chromedriver-win64\\chromedriver.exe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error al hacer scraping'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.config import ConfigScrap\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    global user_id\n",
    "    config = ConfigScrap(**event).__dict__\n",
    "    id =insert_config(config)\n",
    "    user_id = id[\"oki\"][\"lastId_search\"]\n",
    "    df =  scraper.search_people(config)\n",
    "\n",
    "    if df is not None:\n",
    "        df.to_csv(CSV_FILE_PATH, index=False)\n",
    "        return 'Scraping finalizado'\n",
    "    else:\n",
    "        return 'Error al hacer scraping'\n",
    "event ={\n",
    "    \"keyword\": \"java developer\",\n",
    "    \"location\": \"leon\",\n",
    "    \"initial_page\": 1,\n",
    "    \"final_page\": 4,\n",
    "}\n",
    "\n",
    "lambda_handler(event, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas: 40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    return pd.read_csv('profiles.csv')\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = get_data()\n",
    "    #saber el total de filas\n",
    "    total_rows = data.shape[0]\n",
    "    print(f\"Total de filas: {total_rows}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scraper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# medium_config = {\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#     \"keyword\": \"vue\",\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#     \"location\": \"leon\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m#     \"is_search\": True,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# }\u001b[39;00m\n\u001b[0;32m     22\u001b[0m config \u001b[39m=\u001b[39m ConfigScrap(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbasic_config)\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m---> 23\u001b[0m df \u001b[39m=\u001b[39m scraper\u001b[39m.\u001b[39msearch_people(config)\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m   \u001b[39m#cear un archivo csv y guardar la informacion\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scraper' is not defined"
     ]
    }
   ],
   "source": [
    "#run scraper\n",
    "from model.config import ConfigScrap\n",
    "basic_config = {\n",
    "    \"keyword\": \"vue\",\n",
    "    \"location\": \"Leon, Guanajuato\",\n",
    "    \"initial_page\": 1,\n",
    "}\n",
    "\n",
    "# medium_config = {\n",
    "#     \"keyword\": \"vue\",\n",
    "#     \"location\": \"leon\",\n",
    "#     \"initial_page\": 1,\n",
    "#     \"final_page\": 4,\n",
    "# }\n",
    "\n",
    "# search_config = {\n",
    "#   \"keyword\": \"diseñador\",\n",
    "#     \"profiles_to_search\": [\"https://mx.linkedin.com/in/nanadiseno\",\"https://mx.linkedin.com/in/luisfernando-celaya\"],\n",
    "#     \"is_search\": True,\n",
    "# }\n",
    "\n",
    "\n",
    "df = scraper.search_people(config)\n",
    "\n",
    "if df is not None:\n",
    "  #cear un archivo csv y guardar la informacion\n",
    "  df.to_csv(\"data.csv\", index=False)\n",
    "  \n",
    "else:\n",
    "  print(\"No hay datos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
